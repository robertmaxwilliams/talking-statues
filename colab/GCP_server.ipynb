{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/robertmaxwilliams/talking-statues/blob/master/colab/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwdV7me2lKWq"
   },
   "source": [
    "## IMPORTANT\n",
    "Before you run this notebook you need to copy the static files over!\n",
    "foo.html\n",
    "index.css\n",
    "corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZTrkPPtYUHu"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "wYst5iu6YYGs",
    "outputId": "bd52c3bd-c78e-49f2-ce8a-49b1be88f675"
   },
   "outputs": [],
   "source": [
    "# !pip install gpt_2_simple bottle markovify flask-ngrok flask==0.12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "rj60Jj3nYUHz",
    "outputId": "b1f87de6-29fa-43e1-99cf-969d9604dadf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "import os\n",
    "import requests\n",
    "from random import randint as dice\n",
    "import time\n",
    "import markovify\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = \"\\033[95m\"\n",
    "    OKBLUE = \"\\033[94m\"\n",
    "    OKGREEN = \"\\033[92m\"\n",
    "    WARNING = \"\\033[93m\"\n",
    "    FAIL = \"\\033[91m\"\n",
    "    ENDC = \"\\033[0m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    UNDERLINE = \"\\033[4m\"\n",
    "\n",
    "sess = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S1OODX3oYUIB"
   },
   "source": [
    "## Downloading model, starting session, and defining generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0bws2-zY3k0"
   },
   "outputs": [],
   "source": [
    "model_name = \"124M\"\n",
    "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "    print(f\"Downloading {model_name} model...\")\n",
    "    gpt2.download_gpt2(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y3duV5vpY35k",
    "outputId": "e233e2fa-c874-4503-b9d0-eb78594f19e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoint’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoint && cp -r models/124M checkpoint/run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "sXcjSj4hYUID",
    "outputId": "06792ad8-d0c7-42be-b5bb-b4b69ad9bc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Restart session if running this cell again\n",
    "if (sess != None):\n",
    "    gpt2.reset_session(sess)\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "# gpt2.load_gpt2(sess, multi_gpu=False)\n",
    "gpt2.load_gpt2(sess, multi_gpu=True)\n",
    "\n",
    "def generate_text(prefix, num_samples):\n",
    "    prefix = prefix[:100]\n",
    "    output = gpt2.generate(\n",
    "        sess,\n",
    "        prefix=prefix,\n",
    "        include_prefix=False,\n",
    "        return_as_list=True,\n",
    "        length=100,\n",
    "#         batch_size=5,\n",
    "        nsamples=num_samples,\n",
    "    )\n",
    "    output = [x[len(prefix):] for x in output]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGkVvb-FYUIL"
   },
   "source": [
    "## Creating and Starting server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "1SfRZ5ShiT2C",
    "outputId": "2a44daf3-9335-42a0-b3d8-4aee76015c3b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8080/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [30/Mar/2020 17:35:54] \"\u001b[36mGET / HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [30/Mar/2020 17:35:54] \"\u001b[36mGET /static/index.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [30/Mar/2020 17:35:55] \"\u001b[36mGET / HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [30/Mar/2020 17:35:55] \"\u001b[36mGET /static/index.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [30/Mar/2020 17:35:56] \"\u001b[36mGET / HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [30/Mar/2020 17:35:56] \"\u001b[36mGET /static/index.css HTTP/1.1\u001b[0m\" 304 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate: prefix = the bread\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [30/Mar/2020 17:36:06] \"\u001b[37mPOST /generate HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to respond: 8.72697925567627\n"
     ]
    }
   ],
   "source": [
    "from flask_ngrok import run_with_ngrok\n",
    "from flask import Flask, render_template, send_from_directory, request\n",
    "import time\n",
    "\n",
    "app = Flask(__name__)\n",
    "# run_with_ngrok(app) \n",
    "\n",
    "# build text model\n",
    "with open(\"corpus.txt\", encoding=\"utf8\") as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "text_model = markovify.Text(corpus)\n",
    "\n",
    "\n",
    "def generate_from_text_model(text, num_samples=1):\n",
    "    gems = generate_text(text, num_samples)\n",
    "    ret = \"\"\n",
    "    try:\n",
    "        for i in range(num_samples):\n",
    "            ret += f\"<div class='predictionBox'> <p><pre>{gems[i]}</pre></p> </div>\"\n",
    "    except Exception as e:\n",
    "        print(f\"FEEE\\n{e}\\nEEEEF\")\n",
    "        return \"sorry, model failure: \" + str(e)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def random_color():\n",
    "    return \"{:06x}\".format(dice(0, 0xFFFFFF))\n",
    "\n",
    "\n",
    "def colorize(text):\n",
    "    words = text_model.word_split(text)\n",
    "    ret = \"\"\n",
    "    for w in words:\n",
    "        ret += f\"<span style='background-color:#{random_color()}'>{w}</span>\"\n",
    "    return ret\n",
    "\n",
    "\n",
    "@app.route(\"/static/<filename>\")\n",
    "def server_static(filename):\n",
    "    # for now to make uploading static files easier\n",
    "    return send_from_directory(\"./\", filename)\n",
    "    # return send_from_directory(\"./static/\", filename)\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_from_directory(\"./\", \"foo.html\")\n",
    "\n",
    "\n",
    "@app.route(\"/generate\", methods=[\"POST\"])\n",
    "def generate():\n",
    "    generate_start= time.time()\n",
    "    \n",
    "    text = request.form[\"text\"]\n",
    "    print(f\"Generate: prefix = {text}\")\n",
    "    stories = generate_from_text_model(text, num_samples=1)\n",
    "    \n",
    "    generate_end = time.time()\n",
    "    print(f\"Time to respond: {generate_end - generate_start}\")\n",
    "    \n",
    "    return stories\n",
    "\n",
    "\n",
    "@app.route(\"/highlight\", methods=[\"POST\"])\n",
    "def highlight():\n",
    "    time.sleep(0.5)\n",
    "    text = request.form[\"text\"]\n",
    "    print(f\"Highlight: text = {text}\")\n",
    "    return \"<p>\" + colorize(text) + \"</p>\"\n",
    "\n",
    "\n",
    "app.run(port=8080)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "colab_test.ipy",
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
